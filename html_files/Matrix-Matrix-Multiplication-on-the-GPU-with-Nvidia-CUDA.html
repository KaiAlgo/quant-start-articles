
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="google-site-verification" content="wl3-8ed1QZjI0iYZMv10zoZWYElkMObTfwLlWIj9cpA" />
    <meta name="description" content="Matrix-Matrix Multiplication on the GPU with Nvidia CUDA">

    <link rel="icon" href="/static/images/favicon.png">

    <title>Matrix-Matrix Multiplication on the GPU with Nvidia CUDA | QuantStart</title>
    
    
<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,900&display=swap" rel="stylesheet"> 
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700,900&display=swap" rel="stylesheet"> 
<link href="/static/css/bootstrap.min.css" rel="stylesheet">
<link href="/static/css/prism.css" rel="stylesheet">
<link href="/static/css/qs.css?v=10" rel="stylesheet">
    
  </head>

  <body>
    <header class="header covered-header" style="background-image: linear-gradient(rgba(0, 0, 0, 0.8), rgba(0, 0, 0, 0.8)), url(https://quantstartmedia.s3.amazonaws.com/images/article-images/article-backgrounds/default-bg.jpg);">
  
<nav class="nav">
  <div class="container nav-container">
    <div class="nav-row row d-flex justify-content-between align-items-center">
      <div class="col-2">
        <ul class="nav-items justify-content-end small-capitals align-items-center">
          <li class="nav-item">
            <a class="link-fade" href="/">QuantStart</a>
          </li>
        </ul>
      </div>
      <div class="col-auto col-logo">
        <ul id="top-nav-menu" class="nav-items justify-content-end align-items-center">
          
          <li class="nav-item">
            <a class="link-fade" href="/qsalpha/">QSAlpha</a>
          </li>
          
          
          <li class="nav-item">
            <a class="link-fade" href="/quantcademy/">Quantcademy</a>
          </li>
          
          <li id="menu-link-ebooks" class="nav-item">
            <a class="link-fade" href="#">Books</a>
            <div id="menu-pane-ebooks" class="nav-items menu-dropdown-pane">
              <div class="nav-item">
                <a class="link-fade d-block ml-3 mr-3 my-3 mt-4" href="/successful-algorithmic-trading-ebook/">Successful Algorithmic Trading</a>
              </div>
              <div class="nav-item">
                <a class="link-fade d-block ml-3 mr-3 my-3 mt-4" href="/advanced-algorithmic-trading-ebook/">Advanced Algorithmic Trading</a>
              </div>
              <div class="nav-item">
                <a class="link-fade d-block ml-3 mr-3 my-3 mt-4" href="/cpp-for-quantitative-finance-ebook/">C++ For Quantitative Finance</a>
              </div>
            </div>
          </li>
          <li class="nav-item">
            <a class="link-fade" href="/qstrader/">QSTrader</a>
          </li>
          <li class="nav-item">
            <a class="link-fade" href="/articles/">Articles</a>
          </li>
          
          <li class="nav-item">
            <a class="link-fade" href="/members/login/">Login</a>
          </li>
          
        </ul>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
      </div>
    </div>
  </div>
</nav>

<nav id="mobile-nav" class="mobile-nav text-left">
  <div class="container">
    <ul class="mt-4 ml-3 mobile-nav-menu">
      <li class="nav-item">
        <a class="link-fade d-block" href="/">QuantStart</a>
      </li>

      
      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/qsalpha/">QSAlpha</a>
      </li>
      

      
      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/quantcademy/">Quantcademy</a>
      </li>
      

      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="#">Books</a>
      </li>
      <li class="nav-item sub-item">
        <a class="link-fade d-block ml-3" href="/successful-algorithmic-trading-ebook/">Successful Algorithmic Trading</a>
      </li>
      <li class="nav-item sub-item">
        <a class="link-fade d-block ml-3" href="/advanced-algorithmic-trading-ebook/">Advanced Algorithmic Trading</a>
      </li>
      <li class="nav-item sub-item">
        <a class="link-fade d-block ml-3" href="/cpp-for-quantitative-finance-ebook/">C++ For Quantitative Finance</a>
      </li>

      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/qstrader/">QSTrader</a>
      </li>

      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/articles/">Articles</a>
      </li>

      
      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/members/login/">Login</a>
      </li>
      
    </ul>
    <button class="nav-toggle mobile-nav-close">
      <svg id="mobile-nav-close-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
        <path d="M19.77,5.63,13.41,12l6.36,6.37a1,1,0,0,1-1.41,1.41L12,13.41,5.63,19.77a1,1,0,0,1-1.44-1.39l0,0L10.58,12,4.21,5.63a1,1,0,0,1,0-1.42,1,1,0,0,1,1.41,0l0,0L12,10.58l6.37-6.37a1,1,0,0,1,1.41,0A1,1,0,0,1,19.77,5.63Z"></path>
      </svg>
    </button>
  </div>
</nav>

  <div class="container hero-container">
    <section class="mt-5 mb-4">
      <div class="row justify-content-center">
        <div class="col-12 text-center">
          <p class="hero">Matrix-Matrix Multiplication on the GPU with Nvidia CUDA</p>
          <p class="hero subhero">Matrix-Matrix Multiplication on the GPU with Nvidia CUDA</p>
        </div>
      </div>
    </section>
  </div>
</header>
    
<section class="container content-container">
  <div class="row">
    <div class="col-md-8 order-md-2">
      <section class="content article-content">
        
        
        <p>In the <a href="http://www.quantstart.com/articles/Monte-Carlo-Simulations-In-CUDA-Barrier-Option-Pricing">previous article</a> we discussed Monte Carlo methods and their implementation in CUDA, focusing on option pricing.</p>

<p>Today, we take a step back from finance to introduce a couple of essential topics, which will help us to write more advanced (and efficient!) programs in the future.</p>

<p>In subsequent articles I will introduce multi-dimensional thread blocks and shared memory, which will be extremely helpful for several aspects of computational finance, e.g. option pricing under a binomial model and using finite difference methods (FDM) for solving PDEs.</p>

<p>As usual, we will learn how to deal with those subjects in CUDA by coding. In this article we will use a matrix-matrix multiplication as our main guide.</p>

<p>So far you should have read my other articles about starting with CUDA, so I will not explain the "routine" part of the code (i.e. everything not relevant to our discussion). But don't worry, at the end of the article you can find the complete code. Also, if you have any doubt, feel free to ask me for help in the comment section.</p>

<h2>Matrix-Matrix Multiplication</h2>

<p>Before starting, it is helpful to briefly recap how a matrix-matrix multiplication is computed. Let's say we have two matrices, $A$ and $B$. Assume that $A$ is a $n \times m$ matrix, which means that it has $n$ rows and $m$ columns. Also assume that $B$ is a $m \times w$ matrix. The result of the multiplication $A*B$ (which is different from $B*A$!) is a $n \times w$ matrix, which we call $M$. That is, the number of rows in the resulting matrix equals the number of rows of the first matrix $A$ and the number of columns of the second matrix $B$.</p>

<p>Why does this happen and how does it work? The answer is the same for both questions here. Let's take the cell $1$,$1$ (first row, first column) of $M$. The number inside it after the operation $M=A*B$ is the sum of all the element-wise multiplications of the numbers in $A$, row 1, with the numbers in $B$, column 1. That is, in the cell $i$,$j$ of $M$ we have the sum of the element-wise multiplication of all the numbers in the $i$-th row in $A$ and the $j$-th column in $B$.</p>

<p>The following figure intuitively explains this idea:</p>

<p style="text-align:center">
  <img src="https://s3.amazonaws.com/quantstartmedia/images/qs-valerio-mat-mat-fig1.png" width="80%" />
</p>

<p>It should be pretty clear now why matrix-matrix multiplication is a good example for parallel computation. We have to compute every element in $C$, and each of them is independent from the others, so we can efficiently parallelise.</p>

<p>We will see different ways of achieving this. The goal is to add new concepts throughout this article, ending up with a 2D kernel, which uses shared memory to efficiently optimise operations.</p>

<h2>Grids And Blocks</h2>

<p>After the previous articles, we now have a basic knowledge of CUDA thread organisation, so that we can better examine the structure of grids and blocks.</p>

<p>When we call a kernel using the instruction &lt;&lt;&lt; &gt;&gt;&gt; we automatically define a <code>dim3</code> type variable defining the number of blocks per grid and threads per block.</p>

<p>In fact, grids and blocks are 3D arrays of blocks and threads, respectively. This is evident when we define them before calling a kernel, with something like this:</p>

<pre>
<code class="language-cpp">dim3 blocksPerGrid(512, 1, 1)
dim3 threadsPerBlock(512, 1, 1)
kernel&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;()</code>
</pre>

<p>In the previous articles you didn't see anything like that, as we only discussed 1D examples, in which we didn't have to specify the other dimensions. This is because, if you only give a number to the kernel call as we did, it is assumed that you created a dim3 mono-dimensional variable, implying $y=1$ and $z=1$.</p>

<p>As we are dealing with matrices now, we want to specify a second dimension (and, again, we can omit the third one). This is very useful, and sometimes essential, to make the threads work properly.</p>

<p>Indeed, in this way we can refer to both the $x$ and $y$ axis in the very same way we followed in previous examples. Let's have a look at the code:</p>

<pre>
<code class="language-cpp"> int row = blockIdx.y * blockDim.y + threadIdx.y;
 int col = blockIdx.x * blockDim.x + threadIdx.x;</code>
</pre>

<p>As you can see, it's similar code for both of them. In CUDA, <code>blockIdx</code>, <code>blockDim</code> and <code>threadIdx</code> are built-in functions with members <code>x</code>, <code>y</code> and <code>z</code>. They are indexed as normal vectors in C++, so between 0 and the maximum number minus 1. For instance, if we have a grid dimension of <code>blocksPerGrid = (512, 1, 1)</code>, <code>blockIdx.x</code> will range between 0 and 511.</p>

<p>As I mentioned <a href="http://www.quantstart.com/articles/Vector-Addition-Hello-World-Example-with-CUDA-on-Mac-OSX">here</a> the total amount of threads in a single block cannot exceed 1024.</p>

<p>Using a multi-dimensional block means that you have to be careful about distributing this number of threads among all the dimensions. In a 1D block, you can set 1024 threads at most in the x axis, but in a 2D block, if you set 2 as the size of y, you cannot exceed 512 for the x! For example, <code>dim3 threadsPerBlock(1024, 1, 1)</code> is allowed, as well as <code>dim3 threadsPerBlock(512, 2, 1)</code>, but not <code>dim3 threadsPerBlock(256, 3, 2)</code>.</p>

<h2>Linearise Multidimensional Arrays</h2>

<p>In this article we will make use of 1D arrays for our matrixes. This might sound a bit confusing, but the problem is in the programming language itself. The standard upon which CUDA is developed needs to know the number of columns before compiling the program. Hence it is impossible to change it or set it in the middle of the code.</p>

<p>However, a little thought shows that this is not a big issue. We cannot use the comfortable notation <code>A[i][j]</code>, but we won't struggle that much as we already know how to properly index rows and columns.</p>

<p>In fact, the easiest way to linearize a 2D array is to stack each row lengthways, from the first to the last. The following picture will make this concept clearer:</p>

<p style="text-align:center">
  <img src="https://s3.amazonaws.com/quantstartmedia/images/qs-valerio-mat-mat-fig2.png" width="80%" />
</p>

<h2>The Kernel</h2>

<p>Now that we have all the necessary information for carrying out the task, let's have a look at the kernel code. For sake of simplicity we will use square $N \times N$ matrices in our example.</p>

<p>The first thing to do, as we already saw, is to determine the $x$ and $y$ axis index (i.e. row and column numbers):</p>

<pre>
<code class="language-cpp">__global__ void multiplication(float *A, float* B, float *C, int N){
   int ROW = blockIdx.y*blockDim.y+threadIdx.y;
   int COL = blockIdx.x*blockDim.x+threadIdx.x;</code>
</pre>

<p>Then, we check that the row and column total does not exceed the number of actual rows and columns in the matrices. As the threads will access the memory in random order, we have to do this for preventing unnecessary threads from performing operations on our matrices. That is, we are making blocks and grids of a certain size, so that if we don't set our $N$ as a multiple of that size, we would have more threads than we need:</p>

<pre>
<code class="language-cpp">  if (ROW &lt; N && COL &lt; N) {</code>
</pre>

<p>We won't have any problem here as we are using square matrices, but it's always a good idea to keep this in mind. We then have to initialise the temporary variable <code>tmp_sum</code> for summing all the cells in the selected row and column. It is always a good procedure to specify the decimal points and the <code>f</code> even if they are zeroes. So let's write:</p>

<pre>
<code class="language-cpp">    float tmp_sum = 0.0f;</code>
</pre>

<p>Now the straightforward part: as for the CPU code, we can use a <code>for</code> loop for computing the sum and then store it in the corresponding C cell.</p>

<pre>
<code class="language-cpp">        for (int i = 0; i &lt; N; i++) {
            tmpSum += A[ROW * N + i] * B[i * N + COL];
        }
    C[ROW * N + COL] = tmpSum;</code>
</pre>

<p>The host (CPU) code starts by declaring variables in this way:</p>

<pre>
<code class="language-cpp">int main() {
    // declare arrays and variables
    int N = 16;
    int SIZE = N*N;
    vector&lt;float&gt; h_A(SIZE);
    vector&lt;float&gt; h_B(SIZE);
    vector&lt;float&gt; h_C(SIZE);</code>
</pre>

<p>Note that, given the <code>if</code> condition in the kernel, we could set $N$ values that are not necessarily a multiple of the block size. Also, I will make use of the library <code>dev_array</code>, which I discussed in <a href="http://www.quantstart.com/articles/dev_array_A_Useful_Array_Class_for_CUDA">this article</a>.</p>

<p>Now let's fill the matrices. There are several ways to do this, such as making functions for manual input or using random numbers. In this case, we simply use a <code>for</code> loop to fill the cells with trigonometric values of the indices:</p>

<pre>
<code class="language-cpp">    for (int i=0; i&lt;N; i++){
        for (int j=0; j&lt;N; j++){
            h_A[i*N+j] = sin(i);
            h_B[i*N+j] = cos(j);
        }
    }</code>
</pre>

<p>Recalling the "Grids and Blocks" paragraph, we now have to set the <code>dim3</code> variables for both blocks and grids dimensions. The grid is just a <code>BLOCK_SIZE</code> $\times$ <code>BLOCK_SIZE</code> grid, so we can write:</p>

<pre>
<code class="language-cpp">dim3 threadsPerBlock (BLOCK_SIZE, BLOCK_SIZE)</code>
</pre>

<p>As we are not working only with matrices with a size multiple of BLOCK_SIZE, we have to use the <code>ceil</code> instruction, to get the next integer number as our size, as you can see:</p>

<pre>
<code class="language-cpp">int n_blocks = ceil(N/BLOCK_SIZE);
dim3 blocksPerGrid (n_blocks, n_blocks)</code>
</pre>

<p>Note that in this way we will use more threads than necessary, but we can prevent them working on our matrices with the <code>if</code> condition we wrote in the kernel.</p>

<p>This is the general solution showing the reasoning behind it all, but in the complete code you will find a more efficient version of it in <code>kernel.cu</code>.</p>

<p>Now we only have to create the device arrays, allocate memory on the device and call our kernel and, a as result, we will have a parallel matrix multiplication program. Using <code>dev_array</code>, we simply write:</p>

<pre>
<code class="language-cpp">    dev_array&lt;float&gt; d_A(SIZE);
    dev_array&lt;float&gt; d_B(SIZE);
    dev_array&lt;float&gt; d_C(SIZE);</code>
</pre>

<p>for declaring arrays and:</p>

<pre>
<code class="language-cpp">    d_A.set(&h_A[0], SIZE);
    d_B.set(&h_B[0], SIZE);</code>
</pre>

<p>for allocating memory. For getting the device results and copying it on the host, we use the get method instead. Once again, this is simply:</p>

<pre>
<code class="language-cpp">d_C.get(&h_C[0], SIZE);</code>
</pre>

<p>At the bottom of this page you can find the complete code, including performance comparison and error computation between the parallel and the serial code.</p>

<p>In the next article I will discuss the different types of memory and, in particular, I will use the shared memory for speeding-up the matrix multiplication. But don't worry, just after this we will come back to an actual finance application by applying what we learned so far to financial problems.</p>

<h2>Full Code</h2>

<p><code>dev_array.h</code></p>

<pre>
<code class="language-cpp">#ifndef _DEV_ARRAY_H_
#define _DEV_ARRAY_H_

#include &lt;stdexcept&gt;
#include &lt;algorithm&gt;
#include &lt;cuda_runtime.h&gt;

template &lt;class T&gt;
class dev_array
{
// public functions
public:
    explicit dev_array()
        : start_(0),
          end_(0)
    {}

    // constructor
    explicit dev_array(size_t size)
    {
        allocate(size);
    }
    // destructor
    ~dev_array()
    {
        free();
    }

    // resize the vector
    void resize(size_t size)
    {
        free();
        allocate(size);
    }

    // get the size of the array
    size_t getSize() const
    {
        return end_ - start_;
    }

    // get data
    const T* getData() const
    {
        return start_;
    }

    T* getData()
    {
        return start_;
    }

    // set
    void set(const T* src, size_t size)
    {
        size_t min = std::min(size, getSize());
        cudaError_t result = cudaMemcpy(start_, src, min * sizeof(T), cudaMemcpyHostToDevice);
        if (result != cudaSuccess)
        {
            throw std::runtime_error("failed to copy to device memory");
        }
    }
    // get
    void get(T* dest, size_t size)
    {
        size_t min = std::min(size, getSize());
        cudaError_t result = cudaMemcpy(dest, start_, min * sizeof(T), cudaMemcpyDeviceToHost);
        if (result != cudaSuccess)
        {
            throw std::runtime_error("failed to copy to host memory");
        }
    }


// private functions
private:
    // allocate memory on the device
    void allocate(size_t size)
    {
        cudaError_t result = cudaMalloc((void**)&start_, size * sizeof(T));
        if (result != cudaSuccess)
        {
            start_ = end_ = 0;
            throw std::runtime_error("failed to allocate device memory");
        }
        end_ = start_ + size;
    }

    // free memory on the device
    void free()
    {
        if (start_ != 0)
        {
            cudaFree(start_);
            start_ = end_ = 0;
        }
    }

    T* start_;
    T* end_;
};

#endif</code>
</pre>

<p><code>matrixmul.cu</code></p>

<pre>
<code class="language-cpp">#include &lt;iostream&gt;
#include &lt;vector&gt;
#include &lt;stdlib.h&gt;
#include &lt;time.h&gt;
#include &lt;cuda_runtime.h&gt;
#include "kernel.h"
#include "kernel.cu"
#include "dev_array.h"
#include &lt;math.h&gt;

using namespace std;

int main()
{
    // Perform matrix multiplication C = A*B
    // where A, B and C are NxN matrices
    int N = 16;
    int SIZE = N*N;

    // Allocate memory on the host
    vector&lt;float&gt; h_A(SIZE);
    vector&lt;float&gt; h_B(SIZE);
    vector&lt;float&gt; h_C(SIZE);

    // Initialize matrices on the host
    for (int i=0; i&lt;N; i++){
        for (int j=0; j&lt;N; j++){
            h_A[i*N+j] = sin(i);
            h_B[i*N+j] = cos(j);
        }
    }

    // Allocate memory on the device
    dev_array&lt;float&gt; d_A(SIZE);
    dev_array&lt;float&gt; d_B(SIZE);
    dev_array&lt;float&gt; d_C(SIZE);

    d_A.set(&h_A[0], SIZE);
    d_B.set(&h_B[0], SIZE);

    matrixMultiplication(d_A.getData(), d_B.getData(), d_C.getData(), N);
    cudaDeviceSynchronize();

    d_C.get(&h_C[0], SIZE);
    cudaDeviceSynchronize();

    float *cpu_C;
    cpu_C=new float[SIZE];

    // Now do the matrix multiplication on the CPU
    float sum;
    for (int row=0; row&lt;N; row++){
        for (int col=0; col&lt;N; col++){
            sum = 0.f;
            for (int n=0; n&lt;N; n++){
                sum += h_A[row*N+n]*h_B[n*N+col];
            }
            cpu_C[row*N+col] = sum;
        }
    }

    double err = 0;
    // Check the result and make sure it is correct
    for (int ROW=0; ROW &lt; N; ROW++){
        for (int COL=0; COL &lt; N; COL++){
            err += cpu_C[ROW * N + COL] - h_C[ROW * N + COL];
        }
    }

    cout &lt;&lt; "Error: " &lt;&lt; err &lt;&lt; endl;

    return 0;
}</code>
</pre>

<p><code>kernel.h</code></p>

<pre>
<code class="language-cpp">#ifndef KERNEL_CUH_
#define KERNEL_CUH_

void matrixMultiplication(float *A, float *B, float *C, int N);

#endif</code>
</pre>

<p><code>kernel.cu</code></p>

<pre>
<code class="language-cpp">#include &lt;math.h&gt;
#include &lt;iostream&gt;
#include "cuda_runtime.h"
#include "kernel.h"
#include &lt;stdlib.h&gt;

using namespace std;

__global__ void matrixMultiplicationKernel(float* A, float* B, float* C, int N) {

    int ROW = blockIdx.y*blockDim.y+threadIdx.y;
    int COL = blockIdx.x*blockDim.x+threadIdx.x;

    float tmpSum = 0;

    if (ROW &lt; N && COL &lt; N) {
        // each thread computes one element of the block sub-matrix
        for (int i = 0; i &lt; N; i++) {
            tmpSum += A[ROW * N + i] * B[i * N + COL];
        }
    }
    C[ROW * N + COL] = tmpSum;
}


void matrixMultiplication(float *A, float *B, float *C, int N){

    // declare the number of blocks per grid and the number of threads per block
    // use 1 to 512 threads per block
    dim3 threadsPerBlock(N, N);
    dim3 blocksPerGrid(1, 1);
        if (N*N &gt; 512){
            threadsPerBlock.x = 512;
            threadsPerBlock.y = 512;
            blocksPerGrid.x = ceil(double(N)/double(threadsPerBlock.x));
            blocksPerGrid.y = ceil(double(N)/double(threadsPerBlock.y));
        }

    matrixMultiplicationKernel&lt;&lt;&lt;blocksPerGrid,threadsPerBlock&gt;&gt;&gt;(A, B, C, N);
}</code>
</pre>
        
        
        <script async data-uid="6296c27f4b" src="https://weathered-brook-5281.ck.page/6296c27f4b/index.js"></script>
      </section>
    </div>
    <div class="col-md-4 book-card order-md-1">
      
<div class="sidebar-advert-container">
  <a href="/qsalpha/?ref=art">
    <img class="card-img-top" src="/static/images/qsalpha-sidebar-advert-small.png" alt="QSAlpha">
  </a>
  <div class="card mb-4 box-shadow">
    <div class="card-body text-center">
      <h3 class="mb-3"><a class="link-fade" href="/qsalpha/?ref=art">QSAlpha</a></h3>
      <p class="card-text medium-text mb-3">Join the QSAlpha research platform that helps fill your strategy research pipeline, diversifies your portfolio and improves your risk-adjusted returns for increased profitability.</p>
      <div class="d-flex justify-content-center align-items-center">
        <div class="btn-group">
          <a class="btn btn-padded btn-outline-primary btn-lg-cta" href="/qsalpha/?ref=art">Find Out More</a>
        </div>
      </div>
    </div>
  </div>

  <a href="/quantcademy/?ref=art">
    <img class="card-img-top" src="/static/images/quantcademy-sidebar-advert-small.png" alt="Quantcademy">
  </a>
  <div class="card mb-4 box-shadow">
    <div class="card-body text-center">
      <h3 class="mb-3"><a class="link-fade" href="/quantcademy/?ref=art">The Quantcademy</a></h3>
      <p class="card-text medium-text mb-3">Join the Quantcademy membership portal that caters to the rapidly-growing retail quant trader community and learn how to increase your strategy profitability.</p>
      <div class="d-flex justify-content-center align-items-center">
        <div class="btn-group">
          <a class="btn btn-padded btn-outline-primary btn-lg-cta" href="/quantcademy/?ref=art">Find Out More</a>
        </div>
      </div>
    </div>
  </div>

  <a href="/successful-algorithmic-trading-ebook/">
    <img class="card-img-top" src="/static/images/sat-sidebar-advert-small.png" alt="Successful Algorithmic Trading">
  </a>
  <div class="card mb-4 box-shadow">
    <div class="card-body text-center">
      <h3 class="mb-3"><a class="link-fade" href="/successful-algorithmic-trading-ebook/">Successful Algorithmic Trading</a></h3>
      <p class="card-text medium-text mb-3">How to find new trading strategy ideas and objectively assess them for your portfolio using a Python-based backtesting engine.</p>
      <div class="d-flex justify-content-center align-items-center">
        <div class="btn-group">
          <a class="btn btn-padded btn-outline-primary btn-lg-cta" href="/successful-algorithmic-trading-ebook/">Find Out More</a>
        </div>
      </div>
    </div>
  </div>

  <a href="/advanced-algorithmic-trading-ebook/">
    <img class="card-img-top" src="/static/images/aat-sidebar-advert-small.png" alt="Advanced Algorithmic Trading">
  </a>
  <div class="card mb-4 box-shadow">
    <div class="card-body text-center">
      <h3 class="mb-3"><a class="link-fade" href="/advanced-algorithmic-trading-ebook/">Advanced Algorithmic Trading</a></h3>
      <p class="card-text medium-text mb-3">How to implement advanced trading strategies using time series analysis, machine learning and Bayesian statistics with R and Python.</p>
      <div class="d-flex justify-content-center align-items-center">
        <div class="btn-group">
          <a class="btn btn-padded btn-outline-primary btn-lg-cta" href="/advanced-algorithmic-trading-ebook/">Find Out More</a>
        </div>
      </div>
    </div>
  </div>
</div>
    </div>
  </div>
</section>

    

<footer>
  <div class="container container-full">
    <section class="mt-1.5 mb-1">
      <div class="row">
        <div class="col-12 col-sm-12 col-md-6 col-xl-3 mb-1.5">
          <ul class="footer-list">
            <li class="footer-list-title">QuantStart</li>
            <li class="footer-list-link"><a class="link-fade" href="/about/">About</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/articles/">Articles</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/sitemap/">Sitemap</a></li>
          </ul>
        </div>

        <div class="col-12 col-sm-12 col-md-6 col-xl-3 mb-1.5">
          <ul class="footer-list">
            <li class="footer-list-title">Products</li>
            
            <li class="footer-list-link"><a class="link-fade" href="/qsalpha/">QSAlpha</a></li>
            
            
            <li class="footer-list-link"><a class="link-fade" href="/quantcademy/">Quantcademy</a></li>
            
            <li class="footer-list-link"><a class="link-fade" href="/qstrader/">QSTrader</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/successful-algorithmic-trading-ebook/">Successful Algorithmic Trading</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/advanced-algorithmic-trading-ebook/">Advanced Algorithmic Trading</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/cpp-for-quantitative-finance-ebook/">C++ For Quantitative Finance</a></li>
          </ul>
        </div>

        <div class="col-12 col-sm-12 col-md-6 col-xl-3 mb-1.5">
          <ul class="footer-list">
            <li class="footer-list-title">Legal</li>
            <li class="footer-list-link"><a class="link-fade" href="/privacy-policy/">Privacy Policy</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/terms-and-conditions/">Terms &amp; Conditions</a></li>
          </ul>
        </div>

        <div class="col-12 col-sm-12 col-md-6 col-xl-3 mb-1.5">
          <ul class="footer-list">
            <li class="footer-list-title">Social</li>
            <li class="footer-list-link"><a class="link-fade" href="https://twitter.com/quantstart">Twitter</a></li>
            <li class="footer-list-link"><a class="link-fade" href="https://www.youtube.com/channel/UCmVnnZ6Y2TrJtY1eQJN6kWA">YouTube</a></li>
          </ul>
        </div>
      </div>
    </section>

    <div class="row mb-1.5 mt-5">
      <div class="col-12 col-md-9 col-lg-8 col-xl-6">
        <div class="footer-copyright">
          <p>©2012-2023 QuarkGluon Ltd. All rights reserved.</p>
        </div>
      </div>
    </div>
  </div>
</footer>

    
<script src="/static/js/jquery-3.2.1.min.js"></script>
<script src="/static/js/prism.js.min"></script>
<script src="/static/js/bootstrap.min.js"></script>
<script src="/static/js/nav.js"></script>

<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-5383959-5']);
  _gaq.push(['_trackPageview']);

  (function() {
  var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
  ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
  var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>


    
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="/static/js/highcharts/highcharts.js"></script>
<script type="text/javascript">
  num_colours = 10;
</script>
<script src="/static/js/statistics.js"></script>


  </body>
</html>
