
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="google-site-verification" content="wl3-8ed1QZjI0iYZMv10zoZWYElkMObTfwLlWIj9cpA" />
    <meta name="description" content="In this article we demonstrate how to train a perceptron model using the perceptron learning rule. We then provide implementations in Scikit-Learn and TensorFlow with the Keras API.">

    <link rel="icon" href="/static/images/favicon.png">

    <title>Training the Perceptron with Scikit-Learn and TensorFlow | QuantStart</title>
    
    
<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,900&display=swap" rel="stylesheet"> 
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700,900&display=swap" rel="stylesheet"> 
<link href="/static/css/bootstrap.min.css" rel="stylesheet">
<link href="/static/css/prism.css" rel="stylesheet">
<link href="/static/css/qs.css?v=10" rel="stylesheet">
    
  </head>

  <body>
    <header class="header covered-header" style="background-image: linear-gradient(rgba(0, 0, 0, 0.8), rgba(0, 0, 0, 0.8)), url(https://quantstartmedia.s3.amazonaws.com/images/article-images/article-backgrounds/training-the-perceptron-with-scikit-learn-and-tensorflow.jpg);">
  
<nav class="nav">
  <div class="container nav-container">
    <div class="nav-row row d-flex justify-content-between align-items-center">
      <div class="col-2">
        <ul class="nav-items justify-content-end small-capitals align-items-center">
          <li class="nav-item">
            <a class="link-fade" href="/">QuantStart</a>
          </li>
        </ul>
      </div>
      <div class="col-auto col-logo">
        <ul id="top-nav-menu" class="nav-items justify-content-end align-items-center">
          
          <li class="nav-item">
            <a class="link-fade" href="/qsalpha/">QSAlpha</a>
          </li>
          
          
          <li class="nav-item">
            <a class="link-fade" href="/quantcademy/">Quantcademy</a>
          </li>
          
          <li id="menu-link-ebooks" class="nav-item">
            <a class="link-fade" href="#">Books</a>
            <div id="menu-pane-ebooks" class="nav-items menu-dropdown-pane">
              <div class="nav-item">
                <a class="link-fade d-block ml-3 mr-3 my-3 mt-4" href="/successful-algorithmic-trading-ebook/">Successful Algorithmic Trading</a>
              </div>
              <div class="nav-item">
                <a class="link-fade d-block ml-3 mr-3 my-3 mt-4" href="/advanced-algorithmic-trading-ebook/">Advanced Algorithmic Trading</a>
              </div>
              <div class="nav-item">
                <a class="link-fade d-block ml-3 mr-3 my-3 mt-4" href="/cpp-for-quantitative-finance-ebook/">C++ For Quantitative Finance</a>
              </div>
            </div>
          </li>
          <li class="nav-item">
            <a class="link-fade" href="/qstrader/">QSTrader</a>
          </li>
          <li class="nav-item">
            <a class="link-fade" href="/articles/">Articles</a>
          </li>
          
          <li class="nav-item">
            <a class="link-fade" href="/members/login/">Login</a>
          </li>
          
        </ul>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
      </div>
    </div>
  </div>
</nav>

<nav id="mobile-nav" class="mobile-nav text-left">
  <div class="container">
    <ul class="mt-4 ml-3 mobile-nav-menu">
      <li class="nav-item">
        <a class="link-fade d-block" href="/">QuantStart</a>
      </li>

      
      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/qsalpha/">QSAlpha</a>
      </li>
      

      
      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/quantcademy/">Quantcademy</a>
      </li>
      

      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="#">Books</a>
      </li>
      <li class="nav-item sub-item">
        <a class="link-fade d-block ml-3" href="/successful-algorithmic-trading-ebook/">Successful Algorithmic Trading</a>
      </li>
      <li class="nav-item sub-item">
        <a class="link-fade d-block ml-3" href="/advanced-algorithmic-trading-ebook/">Advanced Algorithmic Trading</a>
      </li>
      <li class="nav-item sub-item">
        <a class="link-fade d-block ml-3" href="/cpp-for-quantitative-finance-ebook/">C++ For Quantitative Finance</a>
      </li>

      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/qstrader/">QSTrader</a>
      </li>

      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/articles/">Articles</a>
      </li>

      
      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/members/login/">Login</a>
      </li>
      
    </ul>
    <button class="nav-toggle mobile-nav-close">
      <svg id="mobile-nav-close-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
        <path d="M19.77,5.63,13.41,12l6.36,6.37a1,1,0,0,1-1.41,1.41L12,13.41,5.63,19.77a1,1,0,0,1-1.44-1.39l0,0L10.58,12,4.21,5.63a1,1,0,0,1,0-1.42,1,1,0,0,1,1.41,0l0,0L12,10.58l6.37-6.37a1,1,0,0,1,1.41,0A1,1,0,0,1,19.77,5.63Z"></path>
      </svg>
    </button>
  </div>
</nav>

  <div class="container hero-container">
    <section class="mt-5 mb-4">
      <div class="row justify-content-center">
        <div class="col-12 text-center">
          <p class="hero">Training the Perceptron with Scikit-Learn and TensorFlow</p>
          <p class="hero subhero">In this article we demonstrate how to train a perceptron model using the perceptron learning rule. We then provide implementations in Scikit-Learn and TensorFlow with the Keras API.</p>
        </div>
      </div>
    </section>
  </div>
</header>
    
<section class="container content-container">
  <div class="row">
    <div class="col-md-8 order-md-2">
      <section class="content article-content">
        
        
        <p>In the <a href="https://www.quantstart.com/articles/introduction-to-artificial-neural-networks-and-the-perceptron/">previous article</a> on the topic of <strong>artificial neural networks</strong> we introduced the concept of the <strong>perceptron</strong>. We demonstrated that the perceptron was capable of classifying input data via a linear decision boundary.</p>

<p>However we postponed a discussion on <em>how</em> to calculate the parameters that govern this linear decision boundary. Determining these parameters by means of 'training' the perceptron will be the topic of this article.</p>

<p>We will begin by describing the training procedure. We will note its similarity to a popular optimisation approach in deep learning known as <strong>stochastic gradient descent</strong>. Then we will provide some Python code that demonstrates the training mechanism. This is implemented within the <a href="https://scikit-learn.org">Scikit-Learn</a> library. Finally we will examine the corresponding code in the <a href="https://www.tensorflow.org/">TensorFlow</a> library and see how it differs.</p>

<h2>Training the Perceptron</h2>

<p>Recall from the <a href="https://www.quantstart.com/articles/introduction-to-artificial-neural-networks-and-the-perceptron/">previous article</a> that once suitable weights and bias values were available it was straightforward to classify new input data via the inner product of weights and input components, as well as the step activation function.</p>

<p>Thus far we have neglected to describe how the weights and bias values are found prior to carrying out any classification with the perceptron. This is where a training procedure known as the <strong>perceptron learning rule</strong> comes in.</p>

<p>The perceptron learning rule works by accounting for the prediction error generated when the perceptron attempts to classify a particular instance of labelled input data. In particular the rule amplifies the weights (connections) that lead to a minimisation of the error.</p>

<p>As single training instances are provided to the perceptron a prediction is made. If an incorrect classification is generated—compared to the correct 'ground truth' label—the weights that would have led to a correct prediction are reinforced<sup><a href="#ref-geron2019">[3]</a></sup>.</p>

<p>In this manner the weights are iteratively shifted as more training samples are fed into the perceptron until an optimal solution is found.</p>

<p>Mathematically this procedure is given by the following update algorithm:</p>

<p>
\begin{eqnarray}
w_i^{n+1} = w_i^n + \nu (y - \hat{y}) x_i
\end{eqnarray}
</p>

<p>Where:</p>

<ul>
  <li>$w_i^{n}$ is the $i$th weight at step $n$</li>
  <li>$x_i$ is the $i$th component of the current training input data instance</li>
  <li>$y$ is the correct 'ground truth' classification label for this input data</li>
  <li>$\hat{y}$ is the predicted classification label for this input data</li>
  <li>$\nu$ is the <em>learning rate</em></li>
</ul>

<p>Let's break this formula down into separate terms in order to derive some intuition as to how it works. It states that the new weights at step $n+1$, $w_i^{n+1}$ are given by the sum of the old weights, $w_i^{n}$ at step $n$ plus an additional term $\nu (y - \hat{y}) x_i$.</p>

<p>Since this additional term includes the difference between the predicted value of the outcome $\hat{y}$ and the ground truth $y$, this term will become larger if this difference is more extreme. That is, the weights will be moved further from the old value the larger this difference becomes. This makes sense since if the prediction is far away from the correct labelled value it will be necessary to move the weight further to improve subsequent prediction accuracy.</p>

<p>The other factor in this term is the <em>learning rate</em> $\nu$. This coefficient scales the movement of the weights, so that it can either be significantly reduced or substantially amplified. A small $\nu$ means that even for a large prediction difference, the weights will not shift very much. Correspondingly, a large $\nu$ will mean a significant move of the weights even for a small predictive difference.</p>

<p>The learning rate is an example of a <em>hyperparameter</em> for the model. Determining its optimal value is also necessary. However we will delay the discussion on hyperparameter optimisation until we discuss more complex neural network architectures.</p>

<p>Finally the term is also multiplied by $x_i$. That is, if the $i$th component of the input itself is large, then so is the weight shift, all other factors being equal.</p>

<p>We will now demonstrate this perceptron training procedure in two separate Python libraries, namely Scikit-Learn and TensorFlow.</p>

<p><em>We recently published an article on <a href="https://www.quantstart.com/articles/installing-tensorflow-22-on-ubuntu-1804-with-an-nvidia-gpu/">how to install TensorFlow on Ubuntu against a GPU</a>, which will help in running the TensorFlow code below.</em></p>

<h2>Code Implementation</h2>

<p>In this section we will utilise the National Institute of Diabetes and Digestive and Kidney Diseases diabetes dataset<sup><a href="#ref-smith1988">[4]</a></sup> to test the classification capability of the perceptron.</p>

<p>The dataset contains 768 records with eight diagnostic measurements and an outcome as to whether a patient has diabetes. In the dataset all patients are female, at least 21 years of age, and of <a href="https://en.wikipedia.org/wiki/Pima_people">Pima</a> heritage.</p>

<p>The dataset CSV file can be obtained from the Kaggle site <a href="https://www.kaggle.com/uciml/pima-indians-diabetes-database">here</a>. Note that this file will need to placed in the same directory as the following snippet in order to load the data correctly.</p>

<p><em>We are not going to dwell on the specifics of the dataset here. Rather, we are going to utilise it purely as a means of explaining the training algorithm. If you wish to learn more about the diagnostic measurements and how the data was obtained please see <a href="#ref-smith1988">[4]</a> for more details.</em></p>

<h3>Scikit-Learn</h3>

<p>In the subsequent <code>perc_diabetes_sklearn.py</code> snippet we will utilise Pandas and Scikit-Learn to load the diabetes data and fit a perceptron binary classification model.</p>

<p>The first task is to call the Pandas <code>read_csv</code> method to load the dataset CSV file into a <code>DataFrame</code>, chaining the <code>values</code> method to convert the <code>DataFrame</code> entity into a NumPy matrix, suitable for value extraction in Scikit-Learn.</p>

<p>The features matrix <code>X</code> is defined as the first eight columns of this matrix (it has shape <code>(768, 8)</code>). The outcome vector <code>y</code> is the final column, consisting of 0s for no diabetes and 1s for diabetes.</p>

<p>The perceptron model is then initialised with a particular random seed to ensure reproducible results. The model is then trained with the perceptron learning rule via the <code>fit</code> method.</p>

<p>Finally the mean accuracy score on the <em>same</em> in-sample data is output.</p>

<pre>
<code class="language-python"># perc_diabetes_sklearn.py

import pandas as pd
from sklearn.linear_model import Perceptron


if __name__ == "__main__":
    # Load the Pima diabetes dataset from CSV
    # and convert into a NumPy matrix suitable for
    # extraction into X, y format needed for Scikit-Learn
    diabetes = pd.read_csv('diabetes.csv').values

    # Extract the feature columns and outcome response
    # into appropriate variables
    X = diabetes[:, 0:8]
    y = diabetes[:, 8]

    # Create and fit a perceptron model (with reproducible
    # random seed)
    model = Perceptron(random_state=1)
    model.fit(X, y)

    # Output the (in sample) mean accuracy score
    # of the classification
    print("%0.3f" % model.score(X, y))</code>
</pre>

<p>The output is as follows:</p>

<pre>
<code class="language-python">0.531</code>
</pre>

<p>It can be seen that the classification score is approximately 53%.</p>

<p>This low performance is to be expected. We are essentially trying to ask a single linear threshold unit to fit a linear decision hyperplane through complex eight-dimensional data. Such data is unlikely to present a straightforward linear decision boundary between 'no diabetes' and 'diabetes'.</p>

<h3>TensorFlow & Keras</h3>

<p>We will now attempt to implement the perceptron with the Keras API using the TensorFlow library. The code is slightly more complex than the Scikit-Learn version. However the added complexity in the API will prove beneficial in subsequent articles when we come to model deep neural network architectures.</p>

<h4>Hard Sigmoid Activation Function</h4>

<p>Prior to demonstrating and explaining the corresponding TensorFlow/Keras code for training a single perceptron it is worth highlighting that it is difficult to <em>fully</em> reproduce the perceptron as described in the <a href="https://www.quantstart.com/articles/introduction-to-artificial-neural-networks-and-the-perceptron/">previous article</a>. <em>See <a href="#ref-versloot2019">[6]</a> for a detailed discussion as to why this is so.</em></p>

<p>In essence this is due to the nature of the Keras API, which is designed primarily for deep neural network architectures with <em>differentiable</em> activation functions that produce <em>non-zero gradients</em>. The activation function utilised in the original perceptron is a step function, which is not continuous (and thus not differentiable) at zero. It also leads to zero gradients everywhere else.</p>

<p>Since Keras utilises <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a> as the primary optimisation procedure, it is necessary to involve non-zero gradients if the weights are to be changed when training.</p>

<p>To avoid this problem it is possible to replace the step function activation function with a closely-related function called a <a href="https://paperswithcode.com/method/hard-sigmoid">hard sigmoid</a>. The hard sigmoid is a piecewise linear approximation to the original sigmoid function (an "s-curve"), which is differentiable everywhere except at two points. It still possesses zero gradients for certain parts of the domain but admits non-zero gradients in the middle piecewise linear section.</p>

<p>It turns out that this is sufficient to produce a 'perceptron like' implementation in Keras and TensorFlow.</p>

<h4>TensorFlow Implementation</h4>

<p><em>The intent with demonstrating the corresponding TensorFlow/Keras code in this post is to begin familiarising you with the API used for deep neural networks. Many of the parameters provided to the model creation require significantly more explanation than is possible within this post. Hence we will briefly describe each parameter, but will postpone more comprehensive explanations until we discuss deep neural network architectures in subsequent posts.</em></p>

<p>In the following snippet (<code>perc_diabetes_tensorflow.py</code>) we utilise the same Pima diabetes dataset as was used for Scikit-Learn. It is loaded from CSV in exactly the same manner, being placed into the feature matrix <code>X</code> and the outcome vector <code>y</code>.</p>

<p>The difference in the two implementations begins when we define the perceptron model using the Keras API.</p>

<p>We first create the model using a call to <code>Sequential</code>. This is used to group a linear stack of neural network layers into a single model.</p>

<pre>
<code class="language-python"># Create the 'Perceptron' using the Keras API
model = Sequential()</code>
</pre>

<p>Since we only have a single 'layer' in the perceptron this call may appear to be superfluous. However by implementing it in this manner we are demonstrating a common feature of the Keras API and providing familiarity, which can be leveraged for future deep learning models in subsequent articles.</p> 

<p>We then utilise the <code>add</code> method to add a layer of nodes to the sequential model. In particular we are adding a <code>Dense</code> layer, which means that all nodes in the layer are connected to all of the inputs and outputs. Dense layers are also termed <em>fully connected</em> layers. <em>We will discuss dense neural network layers at length in the subsequent article on multi-layer perceptrons.</em></p>

<pre>
<code class="language-python">model.add(Dense(1, input_shape=(8,), activation=hard_sigmoid, kernel_initializer='glorot_uniform'))</code>
</pre>

<p>The first argument <code>1</code> in the call to <code>Dense</code> is the dimensionality of the output. Since we are attempting to determine whether a patient has diabetes or not, this only needs a single dimension. However the second parameter determines the number of inputs. For the diabetes dataset this is eight—one for each of the feature columns in the CSV file.</p>

<p>We then specify the activation function for the layer as the hard sigmoid.</p>

<p>The <code>kernel_initializer</code> keyword argument is given the <code>'glorot_uniform'</code> value. Since we are training the perceptron with stochastic gradient descent (rather than the perceptron learning rule) it is necessary to intialise the weights with non-zero random values rather than initially set them to zero. <em>This aspect will be discussed in depth in subsequent articles.</em></p>

<p>We then set the loss function to utilise <strong>binary cross-entropy</strong> (see our discussion on cross-entropy <a href="https://www.quantstart.com/articles/Beginners-Guide-to-Decision-Trees-for-Supervised-Machine-Learning/">here</a> for more details), which is the standard loss function for binary classification problems.</p>

<pre>
<code class="language-python">model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])</code>
</pre>

<p>The <code>optimizer</code> keyword argument is set to <code>'adam'</code>. <strong>Adam</strong> is a particular variant of stochastic gradient descent. We will not explain how Adam works in this article but for the purposes of this code snippet it can be thought of as a more computationally efficient variant of stochastic gradient descent.</p>

<p>We then train the model using the Adam stochastic gradient descent algorithm. We utilise the concept of mini-batches, passing in 25 training samples at once. <em>You can read more about mini-batches <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Iterative_method">here</a>.</em></p>

<pre>
<code class="language-python"># Train the perceptron using stochastic gradient descent
# with a validation split of 20%
model.fit(X, y, epochs=225, batch_size=25, verbose=1, validation_split=0.2)</code>
</pre>

<p>The <code>epochs</code> keyword argument determines how many times we iterate over the full training set. For this example we have 225 epochs. It is necessary to iterate over the dataset multiple times in order to mitigate the problem of attaining a local minimum set of values for the weights. Multiple epochs provide a better chance of attaining the global maximum, or a potentially improved local minimum.</p>

<p>In this instance we utilise 20% of the training data as a 'validation' set, which is 'held out' (that is, not trained on) and used solely for evaluating the accuracy of the predictions. We did not do this for the Scikit-Learn implementation and instead checked the accuracy <em>in sample</em>.</p>

<p>Lastly as with the Scikit-Learn implementation we output the final prediction accuracy. Here is the full snippet (slightly modified from versions presented at <a href="#ref-brownlee2019">[5]</a> and <a href="#ref-versloot2019">[6]</a>):</p>

<pre>
<code class="language-python"># perc_diabetes_tensorflow.py

import pandas as pd
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.activations import hard_sigmoid


if __name__ == "__main__":
    # Load the Pima diabetes dataset from CSV
    # and convert into a NumPy matrix suitable for
    # extraction into X, y format needed for TensorFlow
    diabetes = pd.read_csv('diabetes.csv').values

    # Extract the feature columns and outcome response
    # into appropriate variables
    X = diabetes[:, 0:8]
    y = diabetes[:, 8]

    # Create the 'Perceptron' using the Keras API
    model = Sequential()
    model.add(Dense(1, input_shape=(8,), activation=hard_sigmoid, kernel_initializer='glorot_uniform'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Train the perceptron using stochastic gradient descent
    # with a validation split of 20%
    model.fit(X, y, epochs=225, batch_size=25, verbose=1, validation_split=0.2)

    # Evaluate the model accuracy
    _, accuracy = model.evaluate(X, y)
    print("%0.3f" % accuracy)</code>
</pre>

<p>The (truncated) output will be similar to the following:</p>

<pre>
<code class="language-none">..
..
Epoch 214/225
25/25 [==============================] - 0s 2ms/step - loss: 5.3510 - accuracy: 0.6531 - val_loss: 5.5089 - val_accuracy: 0.6429
Epoch 215/225
25/25 [==============================] - 0s 2ms/step - loss: 5.3510 - accuracy: 0.6531 - val_loss: 5.5089 - val_accuracy: 0.6429
Epoch 216/225
25/25 [==============================] - 0s 2ms/step - loss: 5.3510 - accuracy: 0.6531 - val_loss: 5.5089 - val_accuracy: 0.6429
Epoch 217/225
25/25 [==============================] - 0s 2ms/step - loss: 5.3510 - accuracy: 0.6531 - val_loss: 5.5089 - val_accuracy: 0.6429
Epoch 218/225
25/25 [==============================] - 0s 2ms/step - loss: 5.3510 - accuracy: 0.6531 - val_loss: 5.5089 - val_accuracy: 0.6429
Epoch 219/225
25/25 [==============================] - 0s 2ms/step - loss: 5.3510 - accuracy: 0.6531 - val_loss: 5.5089 - val_accuracy: 0.6429
Epoch 220/225
25/25 [==============================] - 0s 2ms/step - loss: 5.3510 - accuracy: 0.6531 - val_loss: 5.5089 - val_accuracy: 0.6429
Epoch 221/225
25/25 [==============================] - 0s 2ms/step - loss: 5.3510 - accuracy: 0.6531 - val_loss: 5.5089 - val_accuracy: 0.6429
Epoch 222/225
25/25 [==============================] - 0s 2ms/step - loss: 5.3510 - accuracy: 0.6531 - val_loss: 5.5089 - val_accuracy: 0.6429
Epoch 223/225
25/25 [==============================] - 0s 2ms/step - loss: 5.3510 - accuracy: 0.6531 - val_loss: 5.5089 - val_accuracy: 0.6429
Epoch 224/225
25/25 [==============================] - 0s 2ms/step - loss: 5.3510 - accuracy: 0.6531 - val_loss: 5.5089 - val_accuracy: 0.6429
Epoch 225/225
25/25 [==============================] - 0s 2ms/step - loss: 5.3510 - accuracy: 0.6531 - val_loss: 5.5089 - val_accuracy: 0.6429
24/24 [==============================] - 0s 793us/step - loss: 5.3827 - accuracy: 0.6510
0.651</code>
</pre>

<p>It can be seen that the final classification score is approximately 65%.</p>

<p>We should view this figure with caution however. We have not fully implemented the perceptron in the same manner as was done with Scikit-Learn. Nor have we evaluated the accuracy in the same way due to the usage of a validation set.</p>

<p>In summary we have carried out the <strong>perceptron learning rule</strong>, using a step function activation function with Scikit-Learn. In the TensorFlow/Keras implementation we carried out <strong>stochastic gradient descent</strong>, using a (mostly) differentiable hard sigmoid activation function. Hence the classification accuracy results will differ.</p>

<p>Despite these differences the intent of the above code has been to provide some insight into the separate APIs of each library. We will be utilising TensorFlow and the Keras API extensively in subsequent articles.</p>

<h2>Next Steps</h2>

<p>We have now implemented and trained our first neural network model in TensorFlow with the Keras API. However such a simplistic model is unlikely to produce effective predication accuracy on more complex data, particularly that utilised within quantitative finance.</p>

<p>In the next article we are going to introduce the <strong>multi-layer perceptron</strong> as a first step in adding more complexity and hence potential predictive accuracy.</p>

<h2>References</h2>

<ul>
  <li><a name="ref-goodfellow2016" href="http://www.deeplearningbook.org/">[1] Goodfellow, I.J., Bengio, Y., Courville, A. (2016) <em>Deep Learning</em>, MIT Press</a></li>
  <li><a name="ref-rosenblatt1958" href="https://psycnet.apa.org/record/1959-09865-001">[2] Rosenblatt, F. (1958) The perceptron: A probabilistic model for information storage and organization in the brain. <em>Psychological Review</em>, <strong>65</strong>, 386-408.</a></li>
  <li><a name="ref-geron2019" href="https://amzn.to/3huiHbo">[3] Géron, A. (2019) <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Ed.</em>, O'Reilly Media</a></li>
  <li><a name="ref-smith1988" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2245318/">[4] Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988) Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. <em>In Proceedings of the Symposium on Computer Applications and Medical Care</em> (pp. 261—265). IEEE Computer Society Press.</a></li>
  <li><a name="ref-brownlee2019" href="https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/">[5] Brownlee, J. (2019) <em>Your First Deep Learning Project in Python with Keras Step-By-Step</em>, Machine Learning Mastery</a></li>
  <li><a name="ref-versloot2019" href="https://www.machinecurve.com/index.php/2019/07/24/why-you-cant-truly-create-rosenblatts-perceptron-with-keras/
">[6] Versloot, C. (2019) <em>Why you can’t truly create Rosenblatt’s Perceptron with Keras</em>, Machine Curve</a></li>
</ul>
        
        
        <script async data-uid="6296c27f4b" src="https://weathered-brook-5281.ck.page/6296c27f4b/index.js"></script>
      </section>
    </div>
    <div class="col-md-4 book-card order-md-1">
      
<div class="sidebar-advert-container">
  <a href="/qsalpha/?ref=art">
    <img class="card-img-top" src="/static/images/qsalpha-sidebar-advert-small.png" alt="QSAlpha">
  </a>
  <div class="card mb-4 box-shadow">
    <div class="card-body text-center">
      <h3 class="mb-3"><a class="link-fade" href="/qsalpha/?ref=art">QSAlpha</a></h3>
      <p class="card-text medium-text mb-3">Join the QSAlpha research platform that helps fill your strategy research pipeline, diversifies your portfolio and improves your risk-adjusted returns for increased profitability.</p>
      <div class="d-flex justify-content-center align-items-center">
        <div class="btn-group">
          <a class="btn btn-padded btn-outline-primary btn-lg-cta" href="/qsalpha/?ref=art">Find Out More</a>
        </div>
      </div>
    </div>
  </div>

  <a href="/quantcademy/?ref=art">
    <img class="card-img-top" src="/static/images/quantcademy-sidebar-advert-small.png" alt="Quantcademy">
  </a>
  <div class="card mb-4 box-shadow">
    <div class="card-body text-center">
      <h3 class="mb-3"><a class="link-fade" href="/quantcademy/?ref=art">The Quantcademy</a></h3>
      <p class="card-text medium-text mb-3">Join the Quantcademy membership portal that caters to the rapidly-growing retail quant trader community and learn how to increase your strategy profitability.</p>
      <div class="d-flex justify-content-center align-items-center">
        <div class="btn-group">
          <a class="btn btn-padded btn-outline-primary btn-lg-cta" href="/quantcademy/?ref=art">Find Out More</a>
        </div>
      </div>
    </div>
  </div>

  <a href="/successful-algorithmic-trading-ebook/">
    <img class="card-img-top" src="/static/images/sat-sidebar-advert-small.png" alt="Successful Algorithmic Trading">
  </a>
  <div class="card mb-4 box-shadow">
    <div class="card-body text-center">
      <h3 class="mb-3"><a class="link-fade" href="/successful-algorithmic-trading-ebook/">Successful Algorithmic Trading</a></h3>
      <p class="card-text medium-text mb-3">How to find new trading strategy ideas and objectively assess them for your portfolio using a Python-based backtesting engine.</p>
      <div class="d-flex justify-content-center align-items-center">
        <div class="btn-group">
          <a class="btn btn-padded btn-outline-primary btn-lg-cta" href="/successful-algorithmic-trading-ebook/">Find Out More</a>
        </div>
      </div>
    </div>
  </div>

  <a href="/advanced-algorithmic-trading-ebook/">
    <img class="card-img-top" src="/static/images/aat-sidebar-advert-small.png" alt="Advanced Algorithmic Trading">
  </a>
  <div class="card mb-4 box-shadow">
    <div class="card-body text-center">
      <h3 class="mb-3"><a class="link-fade" href="/advanced-algorithmic-trading-ebook/">Advanced Algorithmic Trading</a></h3>
      <p class="card-text medium-text mb-3">How to implement advanced trading strategies using time series analysis, machine learning and Bayesian statistics with R and Python.</p>
      <div class="d-flex justify-content-center align-items-center">
        <div class="btn-group">
          <a class="btn btn-padded btn-outline-primary btn-lg-cta" href="/advanced-algorithmic-trading-ebook/">Find Out More</a>
        </div>
      </div>
    </div>
  </div>
</div>
    </div>
  </div>
</section>

    

<footer>
  <div class="container container-full">
    <section class="mt-1.5 mb-1">
      <div class="row">
        <div class="col-12 col-sm-12 col-md-6 col-xl-3 mb-1.5">
          <ul class="footer-list">
            <li class="footer-list-title">QuantStart</li>
            <li class="footer-list-link"><a class="link-fade" href="/about/">About</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/articles/">Articles</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/sitemap/">Sitemap</a></li>
          </ul>
        </div>

        <div class="col-12 col-sm-12 col-md-6 col-xl-3 mb-1.5">
          <ul class="footer-list">
            <li class="footer-list-title">Products</li>
            
            <li class="footer-list-link"><a class="link-fade" href="/qsalpha/">QSAlpha</a></li>
            
            
            <li class="footer-list-link"><a class="link-fade" href="/quantcademy/">Quantcademy</a></li>
            
            <li class="footer-list-link"><a class="link-fade" href="/qstrader/">QSTrader</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/successful-algorithmic-trading-ebook/">Successful Algorithmic Trading</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/advanced-algorithmic-trading-ebook/">Advanced Algorithmic Trading</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/cpp-for-quantitative-finance-ebook/">C++ For Quantitative Finance</a></li>
          </ul>
        </div>

        <div class="col-12 col-sm-12 col-md-6 col-xl-3 mb-1.5">
          <ul class="footer-list">
            <li class="footer-list-title">Legal</li>
            <li class="footer-list-link"><a class="link-fade" href="/privacy-policy/">Privacy Policy</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/terms-and-conditions/">Terms &amp; Conditions</a></li>
          </ul>
        </div>

        <div class="col-12 col-sm-12 col-md-6 col-xl-3 mb-1.5">
          <ul class="footer-list">
            <li class="footer-list-title">Social</li>
            <li class="footer-list-link"><a class="link-fade" href="https://twitter.com/quantstart">Twitter</a></li>
            <li class="footer-list-link"><a class="link-fade" href="https://www.youtube.com/channel/UCmVnnZ6Y2TrJtY1eQJN6kWA">YouTube</a></li>
          </ul>
        </div>
      </div>
    </section>

    <div class="row mb-1.5 mt-5">
      <div class="col-12 col-md-9 col-lg-8 col-xl-6">
        <div class="footer-copyright">
          <p>©2012-2023 QuarkGluon Ltd. All rights reserved.</p>
        </div>
      </div>
    </div>
  </div>
</footer>

    
<script src="/static/js/jquery-3.2.1.min.js"></script>
<script src="/static/js/prism.js.min"></script>
<script src="/static/js/bootstrap.min.js"></script>
<script src="/static/js/nav.js"></script>

<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-5383959-5']);
  _gaq.push(['_trackPageview']);

  (function() {
  var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
  ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
  var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>


    
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="/static/js/highcharts/highcharts.js"></script>
<script type="text/javascript">
  num_colours = 10;
</script>
<script src="/static/js/statistics.js"></script>


  </body>
</html>
